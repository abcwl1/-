#1
from Bio import Entrez
import time
from typing import List, Dict
class PubMedFetcher:
    def __init__(self, email: str):
        Entrez.email = email
    def search_papers(self, query: str, max_results: int = 100) -> List[str]:
        """æœç´¢æ–‡çŒ®è¿”å›PMIDåˆ—è¡¨"""
        handle = Entrez.esearch(
            db="pubmed",
            term=query,
            retmax=max_results,
            sort="relevance"
        )
        record = Entrez.read(handle)
        handle.close()
        return record["IdList"]
    def fetch_abstracts(self, pmid_list: List[str]) -> List[Dict]:
        """æ‰¹é‡è·å–æ‘˜è¦"""
        papers = []
        # åˆ†æ‰¹å¤„ç†ï¼ˆé¿å…APIé™æµï¼‰
        batch_size = 10
        for i in range(0, len(pmid_list), batch_size):
            batch = pmid_list[i:i+batch_size]
            handle = Entrez.efetch(
                db="pubmed",
                id=",".join(batch),
                rettype="abstract",
                retmode="xml"
            )
            records = Entrez.read(handle)
            handle.close()
            for article in records['PubmedArticle']:
                try:
                    medline = article['MedlineCitation']
                    article_data = medline['Article']
                    # æå–å…³é”®ä¿¡æ¯
                    paper = {
                        'pmid': str(medline['PMID']),
                        'title': article_data['ArticleTitle'],
                        'abstract': article_data.get('Abstract', {}).get('AbstractText', [''])[0],
                        'journal': article_data['Journal']['Title'],
                        'year': article_data['Journal']['JournalIssue'].get('PubDate', {}).get('Year', 'N/A'),
                        'authors': self._extract_authors(article_data.get('AuthorList', []))
                    }
                    if paper['abstract']:  # åªä¿ç•™æœ‰æ‘˜è¦çš„
                        papers.append(paper)
                        print(f"âœ… è·å–: {paper['title'][:50]}...")
                except Exception as e:
                    print(f"âŒ è§£æå¤±è´¥: {e}")
            time.sleep(0.5)  # é¿å…é™æµ
        return papers
    def _extract_authors(self, author_list) -> str:
        """æå–ä½œè€…å"""
        authors = []
        for author in author_list[:3]:  # åªå–å‰3ä½
            if 'LastName' in author and 'Initials' in author:
                authors.append(f"{author['LastName']} {author['Initials']}")
        return ", ".join(authors) + (" et al." if len(author_list) > 3 else "")

#2
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document 
from langchain_community.vectorstores import Chroma
from typing import List, Dict

class MedicalRAGBuilder:
    def __init__(self, 
                 embedding_model: str = ""):
        print(f" åŠ è½½Embeddingæ¨¡å‹: {embedding_model}")
        #embeddingæ¨¡å‹
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model, 
            model_kwargs={'device': 'cpu'}  # å¦‚æœæœ‰GPUæ”¹ä¸º'cuda'
        )
        #æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            #é€’å½’å­—ç¬¦åˆ†å‰²ï¼šæŒ‰ä¸åŒçš„å­—ç¬¦é€’å½’åœ°åˆ†å‰²(æŒ‰ç…§separatorsä¸­çš„ä¼˜å…ˆçº§:"\n\n", "\n", "." , â€¦â€¦)
            chunk_size=500,  # æ¯ä¸ªchunkå¤§å°
            chunk_overlap=50,  # ç›¸é‚»chunkä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡ä¸¢å¤±
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""] #åˆ†éš”ç¬¦å­—ç¬¦ä¸²æ•°ç»„
        )
        #å‘é‡æ•°æ®åº“
        self.vectorstore = None

#!rm -rf './chroma_db'  # åˆ é™¤æ—§çš„æ•°æ®åº“æ–‡ä»¶ï¼ˆå¦‚æœæ–‡ä»¶å¤¹ä¸­æœ‰æ–‡ä»¶çš„è¯ï¼‰ï¼Œwindowsç”µè„‘è¯·æ‰‹åŠ¨åˆ é™¤
    def build_vectorstore(self, papers: List[Dict], 
                          #å®šä¹‰æŒä¹…åŒ–è·¯å¾„persist_directory
                          persist_directory: str = ""):
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        documents = []
        for paper in papers:
            # æ„é€ æ–‡æ¡£å†…å®¹
            content = f"Title: {paper['title']}\n\n"
            content += f"PMID: {paper['pmid']}\n\n"
            content += f"Abstract: {paper['abstract']}\n\n"
            content += f"Journal: {paper['journal']} ({paper['year']})\n"
            content += f"Authors: {paper['authors']}"
            # åˆ›å»ºDocumentå¯¹è±¡: from langchain_core.documents import Document
            doc = Document(
                #å†…å®¹page_content
                page_content=content,
                #æè¿°æ€§æ•°æ®metadata
                metadata={
                    'pmid': paper['pmid'],
                    'title': paper['title'],
                    'year': paper['year'],
                    'source': f"https://pubmed.ncbi.nlm.nih.gov/{paper['pmid']}/"
                }
            )
            documents.append(doc)
        print(f" å‡†å¤‡å¯¹ {len(documents)} ç¯‡æ–‡çŒ®è¿›è¡Œå‘é‡åŒ–...")

        # åˆ†å‰²æ–‡æœ¬
        split_docs = self.text_splitter.split_documents(documents)
        print(f"âœ‚ï¸ åˆ†å‰²ä¸º {len(split_docs)} ä¸ªæ–‡æœ¬å—")
        # å‘é‡æ•°æ®åº“
        self.vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=self.embeddings,
            persist_directory=persist_directory
        )
        self.vectorstore.persist()
        print(f"å‘é‡åº“ä¸­å­˜å‚¨çš„æ•°é‡ï¼š{self.vectorstore._collection.count()}")
        print(f"âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {persist_directory}")
    def load_vectorstore(self, 
                         persist_directory: str = ""):
        """åŠ è½½å·²æœ‰çš„å‘é‡æ•°æ®åº“"""
        self.vectorstore = Chroma(
            persist_directory=persist_directory,
            embedding_function=self.embeddings
        )
        print(f"âœ… å‘é‡æ•°æ®åº“å·²åŠ è½½")
#å‘é‡æ£€ç´¢
#å½“ä½ éœ€è¦æ•°æ®åº“è¿”å›ä¸¥è°¨çš„ æŒ‰ä½™å¼¦ç›¸ä¼¼åº¦æ’åºçš„ç»“æœ æ—¶å¯ä»¥ä½¿ç”¨similarity_searchå‡½æ•°ã€‚
    def similarity_search(self, query: str, k: int = 5):
        """ç›¸ä¼¼åº¦æ£€ç´¢"""
        if not self.vectorstore:
            raise ValueError("è¯·å…ˆæ„å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“")
        #è¿”å›æŒ‰ä½™å¼¦ç›¸ä¼¼åº¦æ’åºçš„å‰kä¸ªæ–‡çŒ®ç‰‡æ®µ
        results = self.vectorstore.similarity_search_with_score(query, k=k)
        print(f"\n æ£€ç´¢é—®é¢˜: {query}")
        print(f" æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡çŒ®ç‰‡æ®µ:\n")

        for i, (doc, score) in enumerate(results):
            print(f"æ£€ç´¢åˆ°çš„ç¬¬{i}ä¸ªæ–‡çŒ®ç‰‡æ®µçš„: \n")
            print(f"   ç›¸ä¼¼åº¦: {score:.4f}")
            print(f"   æ ‡é¢˜: {doc.metadata['title']}")
            print(f"   å†…å®¹: {doc.page_content[:200]}...")
            print(f"   æ¥æº: {doc.metadata['source']}\n")
        return results

# é˜²æ­¢AIç”Ÿæˆçš„ç­”æ¡ˆä¸­å¼•ç”¨çš„PMIDä¸å­˜åœ¨
def verify_citations(answer: str, source_docs: List) -> str:
    """éªŒè¯å¹¶ä¿®æ­£å¼•ç”¨"""
    valid_pmids = [doc.metadata['pmid'] for doc in source_docs]
    # æå–ç­”æ¡ˆä¸­çš„PMID
    import re
    mentioned_pmids = re.findall(r'PMID:\s*(\d+)', answer)
    # è¿‡æ»¤æ— æ•ˆå¼•ç”¨
    for pmid in mentioned_pmids:
        if pmid not in valid_pmids:
            answer = answer.replace(f"PMID: {pmid}", "[å¼•ç”¨éªŒè¯å¤±è´¥]")
    return answer

#3.
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
import os
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv())  

class MedicalQASystem:
    def __init__(self, vectorstore):
        """
        åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ
        Args:model_name: å¯é€‰
        """
        self.vectorstore = vectorstore

        #â­æ„å»ºæ£€ç´¢é—®ç­”é“¾
        #question = ""
        retriever = self.vectorstore.as_retriever(search_kwargs={"k": 3})
        #docs = retriever.invoke(question)
        #print(f"æ£€ç´¢åˆ°çš„å†…å®¹æ•°ï¼š{len(docs)}")
        #for i, doc in enumerate(docs):
           #print(f"æ£€ç´¢åˆ°çš„ç¬¬{i}ä¸ªå†…å®¹: \n {doc.page_content}", end="\n-------------\n")
        #â­é…ç½®LLM
        """
        self.llm = OpenAI(
            model_name=model_name,
            temperature=0.3,  # é™ä½éšæœºæ€§ï¼Œæé«˜å‡†ç¡®åº¦
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )"""
        model_name = os.environ['MODEL_NAME']
        api_key = os.environ['API_KEY'] 
        base_url = os.environ['BASE_URL'] 
        # print(f"MODEL_NAME={model_name}, API={api_key}, BASE_URL={base_url}")
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.3,                      # temperatureâ†“ï¼Œéšæœºæ€§å‡å°‘
            openai_api_key=api_key,      
            openai_api_base=base_url
        )
        # â­PromptTemplatesğŸ‘‡
        self.template = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»å­¦æ–‡çŒ®åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡çŒ®å†…å®¹å›ç­”é—®é¢˜ã€‚
è¦æ±‚ï¼š
1. ç­”æ¡ˆå¿…é¡»åŸºäºæä¾›çš„æ–‡çŒ®å†…å®¹
2. å¼•ç”¨å…·ä½“çš„PMIDå’Œæ–‡çŒ®æ ‡é¢˜
3. å¦‚æœæ–‡çŒ®ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
4. ä½¿ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€
5. æ³¨æ„ï¼šæ‰€æœ‰ç¼©å†™æŒ‰åŒ»å­¦æœ¯è¯­è§£é‡Šï¼ˆå¦‚MI=å¿ƒè‚Œæ¢—æ­»ï¼‰
æ–‡çŒ®å†…å®¹ï¼š
{context}
é—®é¢˜ï¼š{question}
è¯·ç»™å‡ºè¯¦ç»†çš„ç­”æ¡ˆï¼š"""
        self.prompt = PromptTemplate(template=self.template)
        #â­æ„å»ºQAé“¾
        self.qa_chain = (
    RunnableParallel(
        {
            "docs": retriever,
            "question": RunnablePassthrough()
        }
    )
    | RunnableParallel(
        {
            "answer": (
                RunnableLambda(
                    lambda x: {
                        "context": "\n\n".join(d.page_content for d in x["docs"]),
                        "question": x["question"],
                    }
                )
                | self.prompt
                | self.llm
                | StrOutputParser()
            ),
            "source_docs": lambda x: x["docs"],
        }
    )
)

    def ask(self, question: str) :# -> Dict
        """
        æé—®å¹¶è·å–ç­”æ¡ˆï¼ˆæ£€ç´¢é—®ç­”é“¾ æ•ˆæœæµ‹è¯•ï¼‰
        Returns:
            {
                'answer': str,
                'sources': List[Dict]
            }
        """
        print(f"\n é—®é¢˜: {question}\n")
        print(" AIæ­£åœ¨æ€è€ƒ...\n")
        self.result = self.qa_chain.invoke(question)
        
        self.answer = self.result["answer"]
        self.source_docs = self.result["source_docs"]
        #ä¼˜åŒ–ï¼šéªŒè¯å¼•ç”¨çš„PMIDæ˜¯å¦çœŸå®å­˜åœ¨
        self.valid_answer = verify_citations(self.answer, self.source_docs)
        
        # æ ¼å¼åŒ–è¾“å‡º
        print(" ç­”æ¡ˆ:")
        print("-" * 80)
        print(self.valid_answer)
        print("-" * 80)
        print("\n å‚è€ƒæ–‡çŒ®:")
        for i, doc in enumerate(self.source_docs, 1):
            title = doc.metadata.get("title", "Unknown title")
            pmid = doc.metadata.get("pmid", "Unknown PMID")
            source = doc.metadata.get("source", "Unknown source")
            print(f"\n[{i}] {title}")
            print(f"    PMID: {pmid}")
            print(f"    æ¥æº: {source}")

        return {
            'answer': self.valid_answer,
            'sources': [doc.metadata for doc in self.source_docs]
        }
    
#å®Œæ•´æµç¨‹
if __name__ == "__main__":
    # Step 1: è·å–æ–‡çŒ®
    fetcher = PubMedFetcher(email="eyl998600@gmail.com")
    pmids = fetcher.search_papers("Alzheimer's disease treatment 2023", max_results=50)
    papers = fetcher.fetch_abstracts(pmids)
    # Step 2: æ„å»ºå‘é‡æ•°æ®åº“
    rag = MedicalRAGBuilder()
    #rag.build_vectorstore(papers, persist_directory="./chroma_db")
    if os.path.exists("./chroma_db"):
       rag.load_vectorstore()
    else: #å¦åˆ™æ¯æ¬¡è¿è¡Œéƒ½ä¼šé‡æ–°å»ºåº“
       rag.build_vectorstore(papers)
    # Step 3: åˆ›å»ºé—®ç­”ç³»ç»Ÿ
    qa_system = MedicalQASystem(rag.vectorstore)
    # Step 4: æé—®
    questions = [
        "What are the most promising treatments for Alzheimer's disease in 2023?",
        "What is the mechanism of action of aducanumab?",
        "Are there any clinical trials showing positive results?"
    ]
    for q in questions:
        qa_system.ask(q)
        print("\n" + "="*100 + "\n")
    

